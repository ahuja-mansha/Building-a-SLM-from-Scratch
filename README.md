# ğŸ§  Small Language Model (SLM) from Scratch  

This project documents the **end-to-end process of building a Small Language Model (SLM)** from foundational principles. Itâ€™s designed as a **hands-on deep dive** into the core components of modern generative AI, offering an educational perspective on what powers large language models (LLMs).  

---

## ğŸš€ Key Features  

- **Tokenizer Implementation**  
  - Built a custom tokenizer from scratch â€” the critical first step to convert raw text into numerical representations for the model.  

- **Transformer Architecture**  
  - Implemented a **scaled-down Transformer** in PyTorch, featuring **multi-head self-attention** and **position-wise feed-forward networks**.  

- **Training Pipeline**  
  - Developed a complete pipeline with **data preparation**, **forward & backward propagation**, **loss computation**, and **performance monitoring**.  

- **Inference Engine**  
  - Created a script to **generate text from the trained model**, showcasing the creative capabilities of the SLM.  

---

## ğŸ› ï¸ Tech Stack  

- **Languages & Libraries:** Python, PyTorch, NumPy  
- **Tools:** Hugging Face (tokenizers)  

---

## ğŸ“Œ Learning Outcomes  

- Gained an in-depth understanding of **how LLMs work under the hood**.  
- Practiced **building ML components from scratch** instead of relying solely on high-level abstractions.  
- Strengthened knowledge of **transformers, tokenization, and training pipelines** in modern NLP.  

---

## ğŸ“‚ Repository Structure  

